{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением\n",
    "\n",
    "Обучение с подкреплением - это класс алгоритмов машинного обучения, в которых происходит обучение автономного агента в некотором окружении, с тем, чтобы максимизировать некоторый долговременный выигрыш. Результатом обучения является политика ( стратегия ), которая ставит в соответствие текущему состоянию среды и агента действие, которое должно быть совершено в следующий момент времени. Обучение с подкреплением эквивалентно задаче оптимального управления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Обучение модели управлению роботизированной рукой\n",
    "\n",
    "![pancake](images/pancake.jpg \"Pancakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE?rel=0&amp;showinfo=0&amp;start=95\" frameborder=\"0\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE?rel=0&amp;showinfo=0&amp;start=95\" frameborder=\"0\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Обучение модели игре в Го\n",
    "\n",
    "![alphago](images/alphago.jpg \"AlphaGo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Управление автономным автомобилем\n",
    "\n",
    "![car](images/gcar.jpg \"Google Self Driving Car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением vs обучение с учителем\n",
    "\n",
    "Принципиальным отличием обучения с подкреплением от обучения с учителем является то, что действия избранные агентом влияют на состояние среды и на те состояния, в которых агент будет оказываться в дальнейшем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploration vs Exploitation Tradeoff\n",
    "\n",
    "Другой фундаментально проблемой, которая встречается в обучении с подкреплением является так называемый Exporation vs Exploitation Tradeoff ( компромисс между исследованием и эксплуатацией среды ). Для иллюстрации данной проблемы мы можем взять смежные с RL задачи - _задачу о секретарше_ и _задачу о многоруком бандите_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача об остановке выбора\n",
    "\n",
    "##### (aka задача о секретарше aka задача о разборчивой невесте)\n",
    "\n",
    "Простейшем примером для иллюстрации этой проблемы является так называемая задача о секретарше.\n",
    "\n",
    "Проблема формулируется так:\n",
    "- Есть множество из N претендентов на одну вакансию / руку принцессы;\n",
    "- Претенденты образуют линейно упорядочное множество, то есть мы можем сравнить любых двух и однозначно определить какой из них лучше \n",
    "- Наниматель / принцесса общается с претендентами в случайном порядке;\n",
    "- С каждым претендентом можно поступить одним из двух способов: согласиться (тогда игра заканчивается), отказать ему (но тогда он больше не вернётся, поэтому уже никогда нельзя будет принять его предложение);\n",
    "\n",
    "Цель нанимателя/принцессы: выбрать самого лучшего из возможных.\n",
    "Вопрос: как следует действовать и с какой вероятностью цель будет достигнута?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Решение задачи об остановке выбора\n",
    "\n",
    "Оптимальным алгоритмом решения этой задачи для случая с заранее известным N будет:\n",
    "\n",
    "* Отказать первым N/e претендентам\n",
    "* Начиная с N/e + 1 претендента согласится с первым претендентом, который будет лучше всех встречавшихся ранее\n",
    "\n",
    "В данном случае вероятность выбрать лучшего претендента будет применрно равна 37% вне зависимости от количества кандидатов. Подробнее можно почитать, например, [здесь](https://www.mccme.ru/mmmf-lectures/books/books/book.25.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача о многоруком бандите\n",
    "\n",
    "Предположим, что у нас есть слот-машина с N рычагами. В базовом случае, каждому из рычагов соответствует некоторое фиксированное распредение выигрышей с неизвестным мат. ожиданием. Каждый раз, когда мы дёргаем ручку мы получаем некоторый случайный выигрыш, распределение которого изначально неизвестно и является разным для каждой ручки. Нашей задачей является определить стратегию, которая максимизирует наш выигрыш в заданом временном периоде. При этом, если мы начнём дёргать только ту ручку, у которой матожидание выигрыша максимально на основании имеющиейся у нас в текущей момент данных, мы рискуем пропустить более выгодную ручку и, тем самым снизить общий выигрыш. С другой стороны - если мы потратим слишком много времени на дёргание всех ручек подряд, наш выигрыш также окажется ниже оптимального.\n",
    "\n",
    "![bandit](images/narmedbandit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Многорукий бандит. Примеры проблем\n",
    "\n",
    "* Клинические исследования\n",
    "* Распределение ресурсов между проектами\n",
    "* Алгоритм ранжирования Яндекса с 2015 года. [Ссылка на статью](http://www.www2015.it/documents/proceedings/proceedings/p1177.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Многорукий бандит. Возможные стратегии\n",
    "\n",
    "Для решения проблемы можно попробовать использовать различные стратегии.\n",
    "\n",
    "- ε-greedy strategy - мы дёргаем за лучший рычаг с вероятностью 1 - ε, и за случайный рычаг с вероятностью ε\n",
    "- ε-first startegy - аналогично задаче о секретарше мы дёргаем за случайный рычаг первые N / ε раз, и далее действуем по жадному алгоритму\n",
    "- Annealed ε-greedy startegy - аналогично ε-greedy, но значение ε уменьшается с течением времени\n",
    "\n",
    "Одним из лучших стратегий основана на оценке верхней границы доверительного интервала. По сути, в каждый момент мы выбираем рычаг с наибольшей верхней границей доверительного интервала. С добавлением новых наград доверительный интервал сужается, и мы переходим к рычагу, который потенциально может быть лучше текущего. Подробности можно почитать [здесь](http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Случайные Марковские процессы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Частично обозримые случайные Марковские процессы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RL Objective"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

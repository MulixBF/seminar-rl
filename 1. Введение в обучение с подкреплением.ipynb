{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением\n",
    "\n",
    "Обучение с подкреплением - это класс алгоритмов машинного обучения, в которых происходит обучение автономного агента в некотором окружении, с тем, чтобы максимизировать некоторый долговременный выигрыш. Результатом обучения является политика ( стратегия ), которая ставит в соответствие текущему состоянию среды и агента действие, которое должно быть совершено в следующий момент времени. Обучение с подкреплением эквивалентно задаче оптимального управления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Обучение модели управлению роботизированной рукой\n",
    "\n",
    "![pancake](images/pancake.jpg \"Pancakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE?rel=0&amp;showinfo=0&amp;start=95\" frameborder=\"0\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE?rel=0&amp;showinfo=0&amp;start=95\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Обучение модели игре в Го\n",
    "\n",
    "![alphago](images/alphago.jpg \"AlphaGo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Alpha Zero\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/chess.png\" style=\"width: 200px;\"/></td>\n",
    "        <td><img src=\"images/shogi.png\" style=\"width: 200px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "| Game  | White     | Black     | Win | Draw | Loss |\n",
    "|-------|-----------|-----------|-----|------|------|\n",
    "| Chess | AlphaZero | Stockfish | 25  | 25   | 0    |\n",
    "| Chess | Stockfish | AlphaZero | 3   | 47   | 0    |\n",
    "| Shogi | AlphaZero | Elmo      | 43  | 2    | 5    |\n",
    "| Shogi | Elmo      | AlphaZero | 47  | 0    | 3    |\n",
    "| Go    | AlphaZero | AG0       | 31  | 19   | -    |\n",
    "| Go    | AG0       | AlphaZero | 29  | 21   | -    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Обучение с подкреплением. Примеры задач\n",
    "\n",
    "#### Управление автономным автомобилем\n",
    "\n",
    "![car](images/gcar.jpg \"Google Self Driving Car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением\n",
    "\n",
    "![diagram](images/rl-diagram.png \"Diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением vs обучение с учителем\n",
    "\n",
    "Принципиальные отличия RL от обучения с учителем\n",
    "\n",
    "* В общем случае обучающая выборка отсутсвует, но есть досуп к сигналу награды \n",
    "* Обратная связь приходит с задержкой\n",
    "* Входные данные представляют из себя последовотельность связанных между собой состояний\n",
    "* Действия агента влияют на последующие входные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Imitation learning\n",
    "\n",
    "Применение алгоритмов для обучения с учителем для задач RL называется Imitation Learning. Алгоритм действий:\n",
    "\n",
    "* Собрать датасет наблюдая за тем, как эксперт решает задачу\n",
    "* Применить алгоритмы обучения с учителем\n",
    "* Использовать полученную политику для выбора действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Imitation Leaning. Проблема\n",
    "\n",
    "В случае задач RL мы можем столкнуться с ситуацией, когда небольшие отклонения в начале нашей траектории приведут к тому, что мы существенно отклонимся от неё, и те данные которые мы будем получать от среды станут существенно отличаться от тех данных, которые модель видела в процессе обучения ( domain shift )\n",
    "\n",
    "![trajectory](images/trajectory.png \"Trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Принцип обучения с подкреплением\n",
    "\n",
    "![rl](images/rl-scheme.png \"Reinforcement Learning Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Марковский процесс принятия решений\n",
    "\n",
    "Марковский процесс - это спецификация задачи последовательного принятия решений для полностью наблюдаемой среды с марковской моделью перехода и дополнительными вознаграждениями. Марковский процесс задаётся кортежем из 4-х множеств:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <ul style=\"text-align: left\">\n",
    "                <li> S - состояния среды </li>\n",
    "                <li> A - действия, которые может совершить агент</li>\n",
    "                <li> P<sub>a</sub>(s, s') - вероятность перехода из одного состояния в другое при действии a </li>\n",
    "                <li> R<sub>a</sub>(s, s') - награда, которую получает агент при переходе между состояниями с заданым действием </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td>![markov](images/mdp.png \"Markov Decision Process\")</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Марковское свойство\n",
    "\n",
    "Важное свойство которое характеризует процессы, которые можно представить в виде MDP - это отсутствие памяти. Это охначает, что распределение будущих состояний условно независимо от предыдущих, при условии текущего состояния. Иными словами - будущее зависит от прошлого только через текущее состояние.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Частично обозримый MDP\n",
    "\n",
    "В случае, когда полное состояние системы недоступно агенту мы будем иметь дело с так называемым частично обозримым марковским процессом принятия решения. Он характеризуется теми же множествами, что и обычный марковский процесс, к которым также прибавляются:\n",
    "\n",
    "* Ω is набор наблюдений\n",
    "* Ο(s, a) набор условных вероятностей, определяющих вероятность наблюдения в зависимости от состояния и действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Finite vs infinite horizon MDP\n",
    "\n",
    "Если мы добавим к марковскому процесу специальное терминально состояние, после которого процесс прекращается, и дальнейшие награды более не поступают мы получим finite horizon MDP. Если же терминальные состояния отсутствуют то мы имеем MDP с бесконечным горизонтом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача RL в терминах MDP\n",
    "\n",
    "Политикой называется функция π(s), которая принимает на вход состояние и возвращает распределение вероятностей действий. Агент выбирает следующее действие в соответствии с результатом этой функци. Задача обучения с подкреплением в данном случае сводиться к тому, чтобы найти такое π, чтобы сумма наград агента действующего в соответствии с этой политикой была максимальна.\n",
    "\n",
    "Траекторией τ называется последовательность состояний и действий предпринятых агентом.\n",
    "\n",
    "Динамикой среды называется распределение $ P(s'| \\space s, a)$. В случае model based алогритмов под моделью подразумевается именно модель динамики окружения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Задача RL в терминах MDP. Дополнительные абстракции\n",
    "\n",
    "* $V_π(s)$ - ( value function ) матожидание суммарной награды в случае, когда агент начинает свою траекторию из состояния s и действует в соответствии с политикой π\n",
    "* $Q_π(s, a)$ - ( action-value function ) матожидание суммарной награды в случае, когда агент начниает свою траекторию из состояния s действием a и в дальнейшем действует в соответствии с политикой π\n",
    "* $A_π(s, a) = Q_π(s, a) - V_π(s)$ - ( advantage function ) - разница между Q-значением и ценностью состояния. Сумма A значений для состояния S равна 0 ( при условии фиксированной политики ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Проблема бесконечной ценности\n",
    "\n",
    "Одной из проблем с которыми мы можем столкнуться при работе с марковскими процессами с бесконечным горизонтом является то, что в подавляющем большинстве подобных процессов V и Q будут стремиться к $\\pm \\infty$, что существенно затрудняет обучение модели. Эта проблема может быть решена модификацией самой задачи - мы можем ввести в процесс специальное терминальное состояние (death state), переход в которое возможен ( с небольшой вероятностью ) из любого другого состояния. Таким образом мы получим задачу с конечным горизонтом и конечной ценностью каждого из состояний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Дисконтируюший множитель\n",
    "\n",
    "Можно показать, что введение death state эквивалентно умножению каждой награды на шаге n на дисконтирующй множитель $γ^n \\space ( 0 \\le γ \\le 1 )$ При γ=0 мы получаем близорукую модель, которая учитывает только награду на текущем шаге. При γ=1 мы возвращаемся к оригинальной. Значения γ чуть менее 1 позволяют решить проболему бесконечной ценности и улучшают сходимость многих алгоритмов RL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploration vs exploitation tradeoff\n",
    "\n",
    "Одной из фундаментальных проблем обучения с подкреплением является так называемой exploration-exploitation tradeoff. Наиболее простой задачей демонстрирующей эту проблему является задача об остановкие выбора ( aka задача о секретарше aka задача о разборчивой невесте ). Она может быть сформулированна следующим образом:\n",
    "\n",
    "* У нас есть одно вакантное место и множество претендентов\n",
    "* Количество претендентов конечно и известно\n",
    "* Претенденты образуют полностью упорядоченное множество ( мы можем сравнить любых двух )\n",
    "* Претенденты интервьюруются последовательно в случайном порядке\n",
    "* Мы можем либо принять претендента. Если мы принимаем претендента игра заканчиваестя, если мы отказываем претендент выбывает из пула и его предложение не может быть принято\n",
    "* Наша задача - выбрать наилучшего претендента с наибольшей вероятностью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Решение задачи об остановке выбора\n",
    "\n",
    "Оптимальной политикой будет отказать первым $π/e$ кандидатам, и принять предложение первого кандидата, который будет лучше всех встреченных до этого. Может быть показано, что вероятность выбрать лучшего кандидаиа стремится к $1/e \\approx 0.368$ при $n \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача о многоруком бандите\n",
    "\n",
    "Предположим, что у нас есть слот-машина с N рычагами. В базовом случае, каждому из рычагов соответствует некоторое фиксированное распредение выигрышей с неизвестным мат. ожиданием. Каждый раз, когда мы дёргаем ручку мы получаем некоторый случайный выигрыш, распределение которого изначально неизвестно и является разным для каждой ручки. Нашей задачей является определить стратегию, которая максимизирует наш выигрыш в заданом временном периоде. При этом, если мы начнём дёргать только ту ручку, у которой матожидание выигрыша максимально на основании имеющиейся у нас в текущей момент данных, мы рискуем пропустить более выгодную ручку и, тем самым снизить общий выигрыш. С другой стороны - если мы потратим слишком много времени на дёргание всех ручек подряд, наш выигрыш также окажется ниже оптимального.\n",
    "\n",
    "![bandit](images/narmedbandit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Многорукий бандит. Примеры проблем\n",
    "\n",
    "* Клинические исследования\n",
    "* Распределение ресурсов между проектами\n",
    "* Алгоритм ранжирования Яндекса с 2015 года. [Ссылка на статью](http://www.www2015.it/documents/proceedings/proceedings/p1177.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Многорукий бандит. Возможные стратегии\n",
    "\n",
    "Для решения проблемы можно попробовать использовать различные стратегии.\n",
    "\n",
    "- ε-greedy strategy - мы дёргаем за лучший рычаг с вероятностью 1 - ε, и за случайный рычаг с вероятностью ε\n",
    "- ε-first startegy - аналогично задаче о секретарше мы дёргаем за случайный рычаг первые N / ε раз, и далее действуем по жадному алгоритму\n",
    "- Annealed ε-greedy startegy - аналогично ε-greedy, но значение ε уменьшается с течением времени\n",
    "\n",
    "Одним из лучших стратегий основана на оценке верхней границы доверительного интервала. По сути, в каждый момент мы выбираем рычаг с наибольшей верхней границей доверительного интервала. С добавлением новых наград доверительный интервал сужается, и мы переходим к рычагу, который потенциально может быть лучше текущего. Подробности можно почитать [здесь](http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Credit assignment problem\n",
    "\n",
    "Часто награда от среды поступает с существенной задержкой ( в худшем случае мы получаем одну единственную награду в конце эпизода ). При этом те действия, которые агент совершал непоследственно перед получением награды никаким образом на ней не влияют. Эта проблема называется credit assignment problem и является одним из самых больших препятствий во многих задачах обучения с подкреплением"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Таксономия алгоритмов обучения с подкрепением\n",
    "\n",
    "![taxonomy](images/taxonomy.png \"Taxonomy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation learning\n",
    "\n",
    " Основной проблемой данного подхода является то, что даже при небольших отклонениях от оригинальной траектории агент попадает в ситуацию, с которой никогда до этого не сталкивался. Эта проблема имеет название Data Mismatch problem\n",
    "\n",
    "![dagger](images/dagger.png \"DAgger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DAgger\n",
    "\n",
    "DAgger ( Dataset Aggregation ) - это алогритм, который позволяет обучать политику с помощью алгоритмов обучения с учителем.\n",
    "\n",
    "![dagger-algo](images/dagger-algo.png \"DAgger algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  DAgger одновременный сбор данных\n",
    "\n",
    "Другой идеей может быть одновременное получение выборки с отклонениями от основной траектории. В качестве примерра можно привести управление автомобилем или дроном, когда сбор данных производится одновременно с нескольких камер, при том, что одна из которых смотрит прямо, а две установленны под некоторым углом, чтобы симулировать ситуацию, когда агент отклонился от основной траектории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/voVxIGyeqgo?rel=0\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/voVxIGyeqgo?rel=0\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nvidia-rig](images/forest-rig.png \"Nvidia Rig\")\n",
    "\n",
    "Машинное обучение с применением лома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradient\n",
    "\n",
    "Одним из наиболее просты подходов к решению задач обучения с подкреплением является прямая оптимизация политики методом градиентного спуска. Мы параметризуем политику π набором параметров $\\theta$ и пытаемся найти значение $\\theta^*$:\n",
    "\n",
    "$\\theta^* = \\text{arg}\\max\\limits_{\\theta}\\, E_{r \\sim p_θ(τ)}  [ \\sum\\limits_{t} r(s_t, a_t) ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# REINFORCE\n",
    "\n",
    "Одним из старейших алгоритмов данного семейства является REINFORCE. Он выводится напрямую:\n",
    "\n",
    "1) В качестве функции потерь берётся:\n",
    "\n",
    "$$J(\\theta) = \\int\\limits_{\\mathbb T} p_{\\theta}(\\tau|\\pi)R(\\tau)d{\\tau}$$\n",
    "\n",
    "2) Её градиент:\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\theta) = \\nabla_\\theta \\int\\limits_{\\mathbb T} p_\\theta(\\tau | \\pi) R(\\tau)d\\tau $$\n",
    "\n",
    "3) Подставляя:\n",
    "\n",
    "$$ \\nabla_\\theta p_\\theta(\\tau|\\pi) = p_\\theta(\\tau|\\pi) \\nabla_\\theta \\log p_\\theta(\\tau|\\pi)$$\n",
    "\n",
    "Получаем:\n",
    "\n",
    "$$J(\\theta) = \\int\\limits_{\\mathbb T} p_\\theta(\\tau|\\pi) \\nabla \\log p_\\theta(\\tau|\\pi) R(\\tau) d\\tau$$\n",
    "$$= E(\\nabla_\\theta \\log p_\\theta(\\tau|\\pi) R(\\tau))$$\n",
    "$$\\approx \\dfrac{1}{K} \\sum\\limits_{k=1}^K \\nabla_\\theta \\log p_\\theta(\\tau_k|\\pi)R(\\tau_k)$$\n",
    "\n",
    "Используя последнее выражение мы можем оптимизировать политику итеративно по сэмплам полученным в результате симуляции.\n",
    "\n",
    "REINFORCE - это акроним REward Increment = Nonnegative Factor \\* Offset Reinforcement \\* Characteristic Eligibility, что описывает шаг алгоритма. На каждом шаге мы обновляем веса политики\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy Gradient vs Maximum Likelihood\n",
    "\n",
    "![polgrad_maxlike](images/polgrad_maxlikelihood.png \"Polgrad vs Maxlike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy Gradient\n",
    "\n",
    "Преимущества:\n",
    "\n",
    "* Хорошо сходится\n",
    "* Может эффективно работать в пространствах действий высокой размерности или с непрерывными действиями \n",
    "* Можно задействовать экспертную демонстрацию\n",
    "* Мы имеем прямой контроль над исследованием среды\n",
    "* Позволяет обучать стохаистические политики\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "* Обычно сходится к локальному, а не глобальному минимуму\n",
    "* Высокая дисперсия результатов выполнения политики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy Gradient. Пример\n",
    "\n",
    "Atari Pong\n",
    "\n",
    "* 8000 эпизодов по ~30 игр\n",
    "* Policy network - двуслойная полносвязная архитектура (пиксели)->( 200 скрытых параметров ) -> (действие) \n",
    "* RMSProp lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YOW8m2YGtRg?rel=0\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YOW8m2YGtRg?rel=0\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value based\n",
    "\n",
    "Мы можем подойти к задаче обучения с подкреплением с другой стороны. Если мы можем предсказать ценность ( сумму наград до конца игры ) каждого состояния или пары состояние-действие, то мы тривиально можем построить оптимальную политику каждый раз выбирая действие, которое либо имеет наибольшую ценность, либо приведёт нас в состояние с наибольшей ценностью.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SARSA\n",
    "\n",
    "Одним из наиболее простых Value-based методом является SARSA. В данном методе мы пытаемся выучить ценность действия в заданном состоянии ( action value ). Обучение происходит в режиме on-policy, то есть мы используем одну и ту же политику для исследования и управления. На каждом шаге мы обновляем значения ценности:\n",
    "\n",
    "$$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)], где $$\n",
    "\n",
    "$\\alpha$ - learning rate\n",
    "\n",
    "$\\gamma$ - дисконтирующий фактор \n",
    "\n",
    "Для обучения мы используем кортеж $ ( s_t, a_t, r_t, s_{t+1}, a_{t+1} ) $ - отсюда название алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Q-learning ( table based )\n",
    "\n",
    "Если мы решим использовать отдельную политику для исследования среды ( учиться в режиме off-policy ) то мы столкнёмся с проблемой - ценность следующего действия теперь зависит от неоптимальной политики, и вместо $Q_{\\pi^*}$ ( ценностью действия при оптимальной политике ) мы выучиваем $Q_{\\pi_{exploratory}}$. Эта проблема может быть легко решена, если для обновления значения Q мы вместо того действия, которое политика избрала в реальности возьмём действие, которое является оптимальным с точки зрения политики. \n",
    "\n",
    "Полученный алгоритм будет называться Q-learning и может эффективно применяется для задач с небольшим дискретным пространством действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DQN\n",
    "\n",
    "В 2013 году небольшой и малоизвестной компанией DeepMind была опубликованна статья описывающая применение нейронной сети для аппроксимации значения Q и обучения агента играм Atari. Авторам статьи удалось добиться результатов превосходящих человеческие на существенной доле игр используя только сырые значения пикселей и очки из игр.\n",
    "\n",
    "До того момента value-based методы с применением нейронных сетей считались слишком нестабильными для практического применения. Авторы статьи применили несколько методов для того, чтобы улучшить сходимость:\n",
    "\n",
    "1. При последовательном обучении модели на результатах симуляции данные в батчах обучающей выборки оказываются сильно скоррелированы. Решением этой проблемы является введение специального кольцевого буфера, в котором хранятся тренировочные данные, и из которого случайным образом сэмплятся батчи для тренировки. Данная техника называется experience replay\n",
    "\n",
    "2. Из-за особенностей алгоритма, при использовании одной и той же сети для пердсказания и управления происходит амплификация шума. Для решения этой проблемы сеть периодически клонируется в так называемую target network, которая и используется для управления агентом. Это позволяет декоррелировать шум и улучшает сходимость.\n",
    "\n",
    "3. Для того, чтобы упростить обучение в качестве тензора состояния использовались несколько последовательных кадров с последующей 3D свёрткой по ним. Это позволяет сети выделять динамические признаки и сильно ускоряет обучение. Вместо этого подхода часто используют RNN\n",
    "\n",
    "4. Стандартной политикой во время тренировки является Annealed ε-greedy policy. Это означает, что агент с вероятностью 1-ε выбирает действие с максимальным предсказанным Q, а с вероятностью ε - случайное действие. При этом сам параметр ε убывает с течением времени по заранее заданному расписанию  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Расширения DQN\n",
    "\n",
    "После публикации оригинальной статьи было предложено несколько расширений оригинального алгоритма:\n",
    "\n",
    "### Double DQN\n",
    "\n",
    "Существенной пробемой DQN является то, что Qnetwork имеет тенденцию переоценивать значения Q. Это происходит из-за того, что для оценки Q использется max в:\n",
    "\n",
    "$$ Q(s, a) \\xrightarrow{} r + \\gamma max_a Q(s', a) $$\n",
    "\n",
    "Для решения этой проблемы можно тренировать две сети, и использовать одну из них для выбора действия, а другую для оценки его значения. При этом, так как мы уже тренируем две сети ( основную и target ) часто для реализации DDQN достаточно просто использовать target network для выбора действия\n",
    "\n",
    "### Deep Attention Q-learning network\n",
    "\n",
    "Во многих задачах добавление механизма attention позволяет улучшить результаты. Для этого мы тренируем отдельную небольшую модель, которая определяет, какие области входных данных являются релевантными - после чего мы используем полченную маску для того, чтобы основная сеть концентрировалась на релевантных для задачи участках. В качестве бонуса мы получаем лёгкий способ визуализации работы сети, который позволяет нам легко понять, какие признаки сеть использует для предсказания\n",
    "\n",
    "### Prioritized Samling\n",
    "\n",
    "Одним из способов улучшить скорость обучения алгоритма - это приотетичированный сэмплинг из replay buffer. Мы увеличиваем верояность сэмплинга тех состояний, на которых ошибка максимальна.\n",
    "\n",
    "### Dueling DQN\n",
    "\n",
    "Идея Dueling DQN заключается в том, что мы декомпозируем action-value Q(s, a) на value V(s) и advantage A(s, a). Это осуществляется архетиктурой, в которой Qnetwork имеет две головы - одна из них считает V(s) пользуясь тем, что эта часть не зависит от действия, а вторая голова считает A(s, a), пользуясь тем, что $\\sum\\limits_a A(s, a) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor Critic\n",
    "\n",
    "Большое распространение в RL получила гибридная архитектура Actor-Critic. Методы данного класса совмещают в себе Policy Gradient и предсказание ценности действий, при этом полученные политики имеют существенно более низкую дисперсию, потому, что для предсказания ценности действия используется более одного примера\n",
    "\n",
    "![actorcritic](images/actor-critic.png \"Actor Critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DDPG\n",
    "\n",
    "После успеха DQN была предложена архитектура DDPG, которая использует основные идеи из DQN в Actor-Critic архитектуре, и позволяет использовать Deep Q-network для управления агентом в задачах с непрерывным пространством действий. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Теорема о детерменистическом градиенте политики \n",
    "\n",
    "Авторы статьи Determenistic Policy Gradient Algorithms показали, что градиент детерминистической политики может быть рассчитан, и что он равен матожиданию градиента функции ценности действия ( Q, action-value function)\n",
    "\n",
    "$$ \\nabla_\\theta(\\mu_\\theta) = \\int\\limits_S \\rho^\\mu(s) \\nabla_\\theta \\mu_\\theta(s) \\nabla_a Q^\\mu(s, a)\\space\\rvert_{a=\\mu_\\theta(s)} ds  $$\n",
    "\n",
    "$$ = \\mathbb E_{s\\sim\\rho^\\mu} [ \\nabla_\\theta\\mu_\\theta(s) \\nabla_a Q^\\mu(s, a)\\rvert_{a=\\mu_\\theta(s)} ] $$\n",
    "\n",
    "Где $\\mu_\\theta$ - детерминистическая политика, $\\theta$ - параметры политики\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DDPG\n",
    "\n",
    "Пользуясь этим результатом авторы статьи про DDPG реализовали Q-learning network в полном соответствии с DQN, и интегрировали её в классический DQN алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# a3c\n",
    "\n",
    "Другим, в крайней степени популярным AC методом является a3c. Название метода расшифровывается как Asynchronous Advantage Actor-Critic. В данном методе:\n",
    "\n",
    "1. Несколько независимых агентов *асинхронно* взаимодействуют с окружением ( каждый со своей копией ) и обновляют параметры собственной копии сети. Это позволяет более оптимально собирать данные, а также автоматически декоррелирует тренировочную выборку\n",
    "\n",
    "2. Мы обучаем сеть предсказывать значение фунцкии A, по аналогии с Dueling DQN\n",
    "\n",
    "3. Мы обновляем веса основной сети двигая из в сторону весов обученных сетей на стороне воркеров по распсанию\n",
    "\n",
    "4. Периодически мы подставляем веса основной сети в воркеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model based methods\n",
    "\n",
    "* Sample efficient\n",
    "* Transferable\n",
    "* Generally have worse performance than model-free methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Оптимальный контроль и планирование \n",
    "\n",
    "Во многих случаях у нас есть модель в которой действует агент. К примеру:\n",
    "\n",
    "* Игры ( шахматы, Го )\n",
    "* Физические эмуляторы\n",
    "* Предсказуемые системы ( к примеру, автомобиль )\n",
    "\n",
    "В таком случае мы можем воспользоваться алгоритмами из теории оптимального управления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal Control Methods\n",
    "\n",
    "* iRLC\n",
    "* Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Выучивание модели\n",
    "\n",
    "* Параметризированная физика\n",
    "* Обучение напрямую из картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Curiosity Based Exploration"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

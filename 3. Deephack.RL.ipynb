{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DeepHack.RL\n",
    "\n",
    "В январе-феврале прошлого года проходил хакатон DeepHack.RL. Задачей было разработать универсального агента, который сможет справиться с играми Atari на которых оригинальный DQN показал наихудшие результаты по сравнению с человеком.\n",
    "\n",
    "![atari](images/atari.jpg \"Atari Games\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Правила\n",
    "\n",
    "* The same agent (algorithm) should be used to learn and generate solutions for all 3 games in finals. The agent should use only information provided by the OpenAI gym environment's \"step\" function for training and generation of solution.\tEnvironment specific handcrafted features and rewards are prohibited.\n",
    "* The agent policy should not be fitted to specific seed of random generator. Method 'env.seed' should not be used during testing of the agent\n",
    "* The agent should not be trained on the human gameplay or initialized with parameters of the third party solution. \n",
    "* The agent should be trained only on games as they are implemented on OpenAI Gym or ALE. During training environments should be initialized with any parameter settings provided here - https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L248:L330. Number of time steps in an environment can be set to any value during training of the agent.\n",
    "* Final solution is generated without interruption of gameplay. Game saving and alternative replays are not allowed during generation of final solutions. This restriction doesn't apply to the training of the agent. \n",
    "* There are no restriction on the means of agent implementation, any learning algorithms and architectures can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Игры \n",
    "\n",
    "## Skiing:\n",
    "\n",
    "![skiing](images/skiing.png \"Skiing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Игры\n",
    "\n",
    "## Ms Pacman:\n",
    "\n",
    "![pacman](images/pacman.jpg \"Pacman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Игры\n",
    "\n",
    "## Centepide:\n",
    "\n",
    "![centepide](images/centepide.jpg \"Centepide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение WPLFF\n",
    "\n",
    "* Чистый A3C с оптимизацией гиперпараметров\n",
    "* Долгое страдание с лыжником"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение 5Vision\n",
    "\n",
    "* N-armed bandit\n",
    "* Monte-Carlo Tree Search and imitation learning\n",
    "* Distribution Mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other\n",
    "\n",
    "* Reward Learning\n",
    "* Double Qlearning with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hq2dVKviowI\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hq2dVKviowI\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
